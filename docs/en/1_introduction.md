# Introduction

Since the introduction of Physics-Informed Neural Networks (PINNs) by Raissi et al. (2019), there has been a significant upsurge in interest in the PINNs technique, spanning various scientific fields. Notably, this technique offers several advantages, such as numerical simplicity compared to conventional schemes. Despite not excelling in terms of performance (accuracy and training computing time), PINNs present a compelling alternative for addressing challenges that prove difficult for traditional methods, such as inverse problems or parametric partial differential equations (PDEs). For comprehensive reviews, refer to Coumo et al. (2022) and Karniadakis et al. (2021).

In this article, I introduce a tutorial on the PINNs technique applied to PDEs containing terms based on the Laplacian in two dimensions (2D), extending the previous tutorial work applied to ordinary differential equations (ODEs) by Baty & Baty (2023). More precisely, I focus on solving different Poisson-type equations called Grad-Shafranov equations and representing magnetic equilibria in magnetically dominated plasmas (e.g. the solar corona), following some examples presented in Baty & Vigon (2024) and also other well known examples (see Cerfon et al. 2011). Additionally, PDEs representative of two dimensional internal star structures are also solved, extending a previous work in a one dimensional approximation (Baty 2023a, Baty 2023b). The latter equations are generally called Lane-Emden equations in the literature. Note that, not only direct problems are considered but also inverse problems for which we seek to obtain an unknown term in the equation. Of course, for these latter problems, additional data on the solutions is required.

I demonstrate how the PINNs technique is particularly well-suited when non Dirichlet-like conditions are imposed at boundaries. This is evident in scenarios involving mixed Dirichlet-Neumann conditions, especially those relevant to the Cauchy problem.

The distinctive feature of the PINNs technique lies in minimizing the residual of the equation at a predefined set of data known as collocation points. At these points, predicted solution is obligated to satisfy the differential equation. To achieve this, a physics-based loss function associated with the residual is formulated and employed. In the original method introduced by Raissi et al. (2019), often referred to as vanilla-PINNs in the literature, the initial and boundary conditions necessary for solving the equations are enforced through another set of data termed training points, where the solution is either known or assumed. These constraints are integrated by minimizing a second loss function, typically a measure of the error like the mean-squared error. This loss function captures the disparity between the predicted solution and the values imposed at the boundaries. The integration of the two loss functions results in a total loss function, which is ultimately employed in a gradient descent algorithm. An advantageous aspect of PINNs is its minimal reliance on extensive training data, as only knowledge of the solution at the boundary is required for vanilla-PINNs. It's worth noting that, following the approach initially proposed by Lagaris (1998), there is another option to precisely enforce boundary conditions to eliminate the need for a traning dataset (see for example Baty 2023b and Urban et al. 2023). This involves compelling the neural networks (NNs) to consistently assign the prescribed value at the boundary by utilizing a well behaved trial function. The use of this latter second option, also refereed as hard-PINNs below, is illustrated in this paper.
